{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Generative AI Model for Dialogue Summarization"
      ],
      "metadata": {
        "id": "AKfVhMYwgyHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a practice notebook by Mark Redito. This is a copy of the same notebook used in the course, [Generative AI with LLMs](https://www.deeplearning.ai/courses/generative-ai-with-llms/) with Redito's comments. All rights are reserved by the authors."
      ],
      "metadata": {
        "id": "Eo5XI5TJM5KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM"
      ],
      "metadata": {
        "id": "S5vcWX7EKFuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will fine-tune an existing LLM from Hugging Face for enhanced dialogue summarization. You will use the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model, which provides a high quality instruction tuned model and can summarize text out of the box. To improve the inferences, you will explore a full fine-tuning approach and evaluate the results with ROUGE metrics. Then you will perform Parameter Efficient Fine-Tuning (PEFT), evaluate the resulting model and see that the benefits of PEFT outweigh the slightly-lower performance metrics."
      ],
      "metadata": {
        "id": "hkndLek-KR3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**1.1 - Set up Kernel and Required Dependencies**"
      ],
      "metadata": {
        "id": "Oh-3-JwTKR_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJUkEAutAIiG"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0 \\\n",
        "    evaluate==0.4.0 \\\n",
        "    rouge_score==0.1.2 \\ #used for eval\n",
        "    loralib==0.1.1 \\ #used for parameter efficient training\n",
        "    peft==0.3.0 --quiet #used for parameter efficient training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing other necessary components:"
      ],
      "metadata": {
        "id": "HpSLdXuHKR6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "KWhtZuG_KZd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoModelForSeq2SeqLM` is our model. `AutoTokenizer` is our tokenizer. `TrainingArguments` and `Trainer` is used to simplify the training of the model."
      ],
      "metadata": {
        "id": "FCKTDqt2KR8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 - Load Dataset and LLM**\n",
        "We are going to experiment with the `DialogSum` Hugging Face dataset. Which containts 10k+ dialogues with manual labeled summaries and topics."
      ],
      "metadata": {
        "id": "25Vflf9cKSBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "id": "WWANcjvWKZgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above also prints out Dataset features and stats such as: id, dialogue, summary, topic, and num_rows"
      ],
      "metadata": {
        "id": "rjGnfkNrKSGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we load the pre-trained FLAN-T5 model and its tokenizer directly from HuggingFace. This will be a small version of the model. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by the model."
      ],
      "metadata": {
        "id": "xD4cQE79KSDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "grYvndqvKZiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, lets print out the model parameters and look at how many of those parameters are trainable."
      ],
      "metadata": {
        "id": "ivzsKZwLNtaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(original_model))"
      ],
      "metadata": {
        "id": "JLgev6vnKZlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the percentage of trainable model parameters are 100%, meaning all of the model weights are trainable."
      ],
      "metadata": {
        "id": "9xkhrzxJN-e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 - Test the Model with Zero Shot Inferencing**"
      ],
      "metadata": {
        "id": "MJnQhxVzOI5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "#below is a prompt template\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "cArhEhPYKZnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that model is decent at summarization and could be better with fine tuning."
      ],
      "metadata": {
        "id": "4yo84ukQOyWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - Perform Full Fine Tuning"
      ],
      "metadata": {
        "id": "sIAwmqa9O9OY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 - Preprocess the Dialog-Summary Dataset**\n",
        "In this stage we need to convert the dialog-summary (prompt and response) pairs into explicit instructions for the model. We prepend an instruction to the start of the dialog with `Summarize the following conversation` and to the start of the summary with `Summary:` as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Summarize the following conversation.\n",
        "  Chris: This is his part of the conversation.\n",
        "  Antje: This is her part of the conversation.\n",
        "\n",
        "Summary:\n",
        "```\n",
        "\n",
        "Training response(Summary):\n",
        "\n",
        "`Both Chris and Antje participated in the conversation.`\n",
        "\n",
        "And then we pre-process the prompt-response dataset into tokens and pull out their `input_ids` (1 per token)\n"
      ],
      "metadata": {
        "id": "o1onqQgmPGN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
      ],
      "metadata": {
        "id": "pKddMjnTKZpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save some time, we will subsample the dataset:"
      ],
      "metadata": {
        "id": "uQJaLawOQX-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "mbWqofKaKZr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check the shapes of all three parts of the dataset:"
      ],
      "metadata": {
        "id": "cB969poHQiGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "GPvmsiIEQlRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is a print out of the dataset. Here we have split the dataset into training (used for training), Validation (to validate the model) and finally Test (which we're holding out for testing). Looks like we're ready for some fine-tuning!"
      ],
      "metadata": {
        "id": "2lbbrNj2QqD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Fine-Tune the Model with the Preprocessed Dataset**\n",
        "\n",
        "We utilize the built-in Hugging Face `Trainer` ([Docs here](https://huggingface.co/docs/transformers/main_classes/trainer)).\n",
        "\n",
        "Notice that we set the `max_steps` and `num_train_epochs` to 1. This saves us time and compute costs for the purpose of this practice notebook.\n",
        "\n",
        "Cool. lets start the training process:"
      ],
      "metadata": {
        "id": "oLl61upMRRNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "OJC0EYI4RJay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets go!!!"
      ],
      "metadata": {
        "id": "HPhEVEh4SS-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "wo5VKDV5SXUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a fully fine-tuned model would take a few hours on a GPU. To save time, we download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model with also be referred to as the **Instruct Model** in this lab."
      ],
      "metadata": {
        "id": "rt1LwjXpSmp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/"
      ],
      "metadata": {
        "id": "s_eeVUaOS5Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the model is ~1Gb"
      ],
      "metadata": {
        "id": "CM6CqxH_S-X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -alh ./flan-dialogue-summary-checkpoint/pytorch_model.bin"
      ],
      "metadata": {
        "id": "xsBnzOM7TCGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:\n",
        "Notice that we use `bfloat16` for compute efficiency."
      ],
      "metadata": {
        "id": "DpoAWEDKTKtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-dialogue-summary-checkpoint\", torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "3qV4mCK1TeWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 - Evaluate the Model Qualitatively (Human Evaluation)**\n",
        "\n",
        "\"Is my model behaving the way it is supposed to?\" is usually a good starting point."
      ],
      "metadata": {
        "id": "YDgc9hLATwJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')"
      ],
      "metadata": {
        "id": "rnooP3sZUNMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the prompts above gives a sense of the model's output in relation to Baseline Human Summary and the Original Model. It seems like the Instruct Model (The model that was finetuned), has better responses."
      ],
      "metadata": {
        "id": "lcyZWvtHUeLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 - Evaluate the Model Quantitavely (with ROUGE Metric)**\n",
        "\n",
        "The [Rogue metric](https://en.wikipedia.org/wiki/ROUGE_(metric)) helps quantify the validity of summarizations procued by models. It compares summarazations to a baseline summary which is usually created by a human. While imperfect, it seems to indicate the overall increate in summarization effectiveness (capturing nuances and details) that we achieved through fine-tuning."
      ],
      "metadata": {
        "id": "fAjgckufU0jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')"
      ],
      "metadata": {
        "id": "eHD05krwUrbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets generate the outputs for the sample of the test dataset. In this case 10 dialogues and summaries to save time. Then we save the results."
      ],
      "metadata": {
        "id": "bkewWEfjVlMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "\n",
        "for _, dialogue in enumerate(dialogues):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "ExUdOwb2Vwf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the output above shows a table that compares `human_baseline_summaries` with `original_model_summaries` and `instruct_model_summaries`.\n",
        "\n",
        "Next we evaluate the model through ROGUE metrics. Across the ROGUE metrics, there seems to be a great improvement."
      ],
      "metadata": {
        "id": "R4AE-CrFV0MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.read_csv(\"data/dialogue-summary-training-results.csv\")\n",
        "\n",
        "human_baseline_summaries = results['human_baseline_summaries'].values\n",
        "original_model_summaries = results['original_model_summaries'].values\n",
        "instruct_model_summaries = results['instruct_model_summaries'].values\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)"
      ],
      "metadata": {
        "id": "ObNNdyNEWxhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, it seems like our model greatly improved its ROGUE scores."
      ],
      "metadata": {
        "id": "vdVgZM1ZXOhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of INSTRUCT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(instruct_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(instruct_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "rSaebwKyXSjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the code above shows the absolute percentage improvement of Instruct Model over Original Model. Looks pretty good!"
      ],
      "metadata": {
        "id": "N8ZqH8LwXZdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Perform Parameter Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "Now, lets perform PEFT. In comparison to the approach above which was \"full fine-tuning\", PEFT is much more efficient (low compute resources) and yields comparable results as full fine tuning.\n",
        "\n",
        "\n",
        "Additional notes from the og notebook:\n",
        "\n",
        "\n",
        "> PEFT is a generic term that includes **Low-Rank Adaptation (LoRA)** and prompt tuning (which is NOT THE SAME as prompt engineering!). In most cases, when someone says PEFT, they typically mean LoRA. LoRA, at a very high level, allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU). After fine-tuning for a specific task, use case, or tenant with LoRA, the result is that the original LLM remains unchanged and a newly-trained “LoRA adapter” emerges. This LoRA adapter is much, much smaller than the original LLM - on the order of a single-digit % of the original LLM size (MBs vs GBs).  \n",
        "\n",
        "> That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request.  The benefit, however, is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases.\n"
      ],
      "metadata": {
        "id": "xyYWno6QXmzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 - Setup the PEFT/LoRA model for Fine-Tuning**\n",
        "\n",
        "Lets setup the PEFT/LoRA. We are going to \"freeze\" the underlying LLM and only train the adapter. Notice the rank `r=32` hyper-parameter. This defines the rank/dimension of the adapter to be trained. *Typically this is a lower number but we'll see what results we get from this.*"
      ],
      "metadata": {
        "id": "orix5vVXYn5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ],
      "metadata": {
        "id": "Xau6fPTqXkzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we add the LoRA adapter layers/parameters to the original LLM to be trained."
      ],
      "metadata": {
        "id": "rmFdXpX6ZSZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model,\n",
        "                            lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "iXhdtaFAZguY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you notice from the output above. The percentage of the trainable model parameters is 1.41%. Compare that to the trainable parameters we did previously (above scroll up) for full fine-tuning, which was 100%. This saves us a lot of time and compute resources."
      ],
      "metadata": {
        "id": "AVe_pxquZj-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 - Train PEFT Adapter**\n",
        "\n",
        "Here, lets define some arguments and create a `trainer` instance."
      ],
      "metadata": {
        "id": "qUV6C9ncaGBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        ")"
      ],
      "metadata": {
        "id": "tA-45nBsaCM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, were good to go. Lets hit train!"
      ],
      "metadata": {
        "id": "jU3VYcJ6aUAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "bZlwCOsiaajI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training performed above was perfomed only on a subset of data. Tull load a fully trained PEFT model, we need to load a checkpoint of a PEFT model from S3"
      ],
      "metadata": {
        "id": "q7U82RxTaetm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ ./peft-dialogue-summary-checkpoint-from-s3/"
      ],
      "metadata": {
        "id": "0RDTvnOGa7t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets look at the size of this model compared to the original model:"
      ],
      "metadata": {
        "id": "2HfLBRE7bGGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al ./peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin"
      ],
      "metadata": {
        "id": "Xd81UcSrbQGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets prepare this model by adding an adapter to the original FLAN-T5 model. We are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. In another case of preparing the model for further training, we wouldld set this as `is_trainable=True`."
      ],
      "metadata": {
        "id": "WMZbU2VPbSl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       './peft-dialogue-summary-checkpoint-from-s3/',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False)"
      ],
      "metadata": {
        "id": "kCExt6KFb3hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of trainable parameters is 0 because we set `is_trainable=False`."
      ],
      "metadata": {
        "id": "KYQzfmFXcBdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "QxPTLOx2cWGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 - Evaluate the Model Qualitatively (Human Evaluation)**\n",
        "\n",
        "Ok, lets test this model out:"
      ],
      "metadata": {
        "id": "mnJKyR33cjQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "baseline_human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: {peft_model_text_output}')"
      ],
      "metadata": {
        "id": "PCvm3Enqcsn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model output is not bad. There's a little more detail to the summary."
      ],
      "metadata": {
        "id": "xxTnK-2Tc3gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.4 - Evaluate the Model Quantitatively (with ROGUE Metric)**"
      ],
      "metadata": {
        "id": "VaGQL0ODdsBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary: \"\"\"\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    human_baseline_text_output = human_baseline_summaries[idx]\n",
        "\n",
        "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    original_model_summaries.append(original_model_text_output)\n",
        "    instruct_model_summaries.append(instruct_model_text_output)\n",
        "    peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "RNkJxA8Hdp83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output above shows a table comparing `human_baseline_summaries`, `original_model_summaries`, `instruct_model_summaries` and `peft_model_summaries`. I noticed that the outputs of the `peft_model_summaries` and `instruct_model_summaries` are quite similar to each other."
      ],
      "metadata": {
        "id": "NEHjEUJGesxb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, now lets compute the ROGUE score for this subset of data."
      ],
      "metadata": {
        "id": "jEMCXK_jfM6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ],
      "metadata": {
        "id": "MERDpClQfKSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we find that the PEFT model score and INSTRUCT model score are close to each other. Pretty impressive performance boost from PEFT while not requiring the same compute resources as INSTRUCT. The training process was also much more easier!"
      ],
      "metadata": {
        "id": "2tYQpPDbfV8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "human_baseline_summaries = results['human_baseline_summaries'].values\n",
        "original_model_summaries = results['original_model_summaries'].values\n",
        "instruct_model_summaries = results['instruct_model_summaries'].values\n",
        "peft_model_summaries     = results['peft_model_summaries'].values\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ],
      "metadata": {
        "id": "qrNwDJ4WfoWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show not much improvement from full fine-tuning but the benefits of PEFT outweigh the slightly lower performance metrics.\n",
        "\n",
        "Now lets calculate the improvement of PEFT over the original model:"
      ],
      "metadata": {
        "id": "ads6q-K9gFNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "zEC2sIgTgMuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty good improvement!\n",
        "\n",
        "Now lets calculate the improvement of PEFT over a full fine tuned model:"
      ],
      "metadata": {
        "id": "2hkccOVQgcwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n",
        "\n",
        "improvement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\n",
        "for key, value in zip(peft_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "aOnNubY-gi2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Percentage improvement of PEFT MODEL over the INSTRUCT MODEL, shows a slight decrease. But considering that PEFT requires less computing and memory resources is a good tradeoff.\n",
        "\n",
        "This shows how PEFT can be a better approach for practitioners who are constrained with compute and memory resources. I'll definitely keep exploring using PEFT in future LLM projects!"
      ],
      "metadata": {
        "id": "_GYfX74ygrfq"
      }
    }
  ]
}